---
title: "Predicting daily Citibike trips"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)

#helper method to load trips and reshape them
loadTrips <- function(location)
{
  load(location)
  temp.trips <- trips %>% 
    group_by(ymd) %>% 
    summarize(count=n()) %>% 
    left_join(weather,by="ymd") %>% 
    filter(!is.na(tmin)) %>%
    mutate(weekday = weekdays(ymd), month = month(ymd)) %>%
    mutate(weekend=(weekday == "Sunday" | weekday == "Saturday"))
  

  #adding lagged weather data
  temp.trips.lagged <- temp.trips %>% mutate (ymd = ymd+1)
  temp.trips.lagged <- merge(temp.trips.lagged,temp.trips, by="ymd", suffixes=c(".prev",""))
  return (temp.trips.lagged)
}

#loading 2015 data
trips.2015 <-loadTrips("2015/trips.RData")
#loading 2014 data
trips.2014 <-loadTrips("2014/trips.RData")
```


#### The point of this exercise is to get experience in an open-ended prediction exercise: predicting the total number of Citibike trips taken on a given day. Do all of your work in an RMarkdown file named predict_citibike.Rmd. Here are the rules of the game: 

1. You can use any features you like that are available prior to the day in question, ranging from the weather, to the time of year and day of week, to activity in previous days or weeks, but don't cheat and use features from the future (e.g., the next day's trips).
    
    ```{r test}
    
    #returns a list of models based on a given formula
    trainModel <- function(formulas, training_data)
    {
      results <- list();
      for (i in length(formulas))
      {
        results[[i]] <- lm(data=training_data, formula=formulas[[i]]);
      }
      return (results);      
    }
    
    #splits the data into multiple parititions
    kfold <- function(data, k)
    {
      training <- list();
      test <- list();
      temp.random <- sample(data);
      lineCount <- 1:nrow(data)%%k+1;
      
      temp.ranked <- cbind(temp.random,p=lineCount)
      for (i in 1:k)
      {
        test[[i]] <- temp.ranked %>% filter(p==i);
        training[[i]] <- temp.ranked %>% filter(p != i);
        
      }
      return (list(training=training,test=test))
    }


    ```
2. As usual, split your data into training and testing subsets and evaluate performance on each.

```{r}
#this will split the data into 5 separate sets of training and test data.
#e.g. data$training[[1]] will give you the trainning data for the first set and data$test[[1]] will give you the test data for the first set.
set.seed(1);
data <- kfold(trips.2014,5) 

```
3. Quantify your performance in two ways: R^2 (or the square of the correlation coefficient), as we've been doing, and with root mean-squared error.

```{r}
    #tests a given list of models against a given list of test_datas
    testModel <- function(models, test_datas)
    {
      results <- list();
      counter <- 1;
      for (i in 1:length(models))
      {
        results[[i]] <- list();
        model <- models[[i]];
        for (j in 1:length(test_datas))
        {
          data <- test_datas[[j]];
          output <- toString(model$terms[[2]]);
          yHat <- predict(model,data);
          rSquared <- cor(yHat,data[[output]])^2;
          rsme <- sqrt(mean((yHat-data[[output]])^2));
          results[[i]][[j]] <-c(rSquared,rsme);
        }
      }
      return (results);
    }

    formulas <- list(
      count~
        poly(tmin,2) + 
        snwd.prev + 
        poly(count.prev,2) + 
        prcp + 
        weekend + 
        snow.prev + 
        I(snwd^5) + I(snwd^3) + I(snwd^2) + snwd)

    #train the model
    models <- trainModel(formulas,data$training[[1]])

   
    testDatas <- c()
    testDatas[[1]] <- data$test[[1]]; #add data from the test data for 2014
    testDatas[[2]] <- trips.2015;  #add data from 2015 as test data
    
    result <- testModel(models,testDatas) 
```
4. Report the model with the best performance on the test data. Watch out for overfitting.

```{r}
    #our best formula so far
    print(paste("best formula so far: ",toString(formulas[[1]][[3]])))

    print(paste("rSquared for 2014 test data(20% of total data): ",result[[1]][[1]][[1]]))
    print(paste("rmse for 2014 test data(20% of total data): ",result[[1]][[1]][[2]]))
    print(paste("rSquared for 2015 test data: ",result[[1]][[2]][[1]]))
    print(paste("rmse for 2015 test data: ",result[[1]][[2]][[2]]))

```
5. Plot your final best fit model in two different ways. First with the date on the x-axis and the number of trips on the y-axis, showing the actual values as points and predicted values as a line. Second as a plot where the x-axis is the predicted value and the y-axis is the actual value, with each point representing one day.
```{r}
#Plot 1. date vs number of trips
trips.2015 %>% 
  mutate(prediction = predict(models[[1]], trips.2015)) %>%
  ggplot(aes(x=ymd)) +
  geom_point(aes(y=count)) +
  geom_line(aes(y=prediction),color="blue")

#plot 2. actual data vs estimated data

trips.2015 %>%
  mutate(prediction = predict(models[[1]], trips.2015)) %>%
  ggplot(aes(x=count,y=prediction)) + geom_point()
```
6. Inspect the model when you're done to figure out what the highly predictive features are, and see if you can prune away any negligble features that don't matter much.